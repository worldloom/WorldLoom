{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "worldloom-header"
   },
   "source": [
    "# ðŸŒ WorldLoom Quick Start\n",
    "\n",
    "**Unified interface for latent world models (DreamerV3, TD-MPC2)**\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-worldloom-blue?style=flat&logo=github)](https://github.com/yoshihyoda/worldloom)\n",
    "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Installation & Setup\n",
    "2. Creating World Models (one-liner API)\n",
    "3. Encoding & Decoding Observations\n",
    "4. Imagination Rollouts (the core feature!)\n",
    "5. Simple Training Demo\n",
    "6. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation-header"
   },
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install WorldLoom directly from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-worldloom"
   },
   "outputs": [],
   "source": "# Install WorldLoom with training dependencies\n!pip install -q \"worldloom[training,viz] @ git+https://github.com/yoshihyoda/worldloom.git\"\n\n# Verify installation\nimport worldloom\n\nprint(f\"WorldLoom version: {worldloom.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# WorldLoom imports\n",
    "from worldloom import create_world_model, list_models\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "models-header"
   },
   "source": [
    "## 2. Available Models\n",
    "\n",
    "WorldLoom provides a unified interface for multiple world model architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list-models"
   },
   "outputs": [],
   "source": [
    "# List all available model presets\n",
    "models = list_models(verbose=True)\n",
    "\n",
    "print(\"Available World Models:\")\n",
    "print(\"=\" * 60)\n",
    "for name, info in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Params: {info['params']}\")\n",
    "    print(f\"  Type: {info['type']}\")\n",
    "    print(f\"  Description: {info['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create-model-header"
   },
   "source": [
    "## 3. Create a World Model (One-Liner!)\n",
    "\n",
    "The `create_world_model()` function provides a simple, unified interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-dreamer"
   },
   "outputs": [],
   "source": [
    "# Create a DreamerV3 model for image observations\n",
    "# obs_shape: (channels, height, width) for images\n",
    "dreamer = create_world_model(\n",
    "    \"dreamerv3:size12m\",\n",
    "    obs_shape=(3, 64, 64),  # RGB images\n",
    "    action_dim=4,  # 4 discrete actions\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"DreamerV3 Model:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in dreamer.parameters()):,}\")\n",
    "print(f\"  Obs shape: {dreamer.config.obs_shape}\")\n",
    "print(f\"  Action dim: {dreamer.config.action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-tdmpc"
   },
   "outputs": [],
   "source": [
    "# Create a TD-MPC2 model for state observations\n",
    "# Perfect for continuous control tasks like MuJoCo\n",
    "tdmpc = create_world_model(\n",
    "    \"tdmpc2:5m\",\n",
    "    obs_shape=(39,),  # State vector (e.g., HalfCheetah has 39-dim state)\n",
    "    action_dim=6,  # 6 continuous actions\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"TD-MPC2 Model:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in tdmpc.parameters()):,}\")\n",
    "print(f\"  Obs shape: {tdmpc.config.obs_shape}\")\n",
    "print(f\"  Action dim: {tdmpc.config.action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "encode-decode-header"
   },
   "source": [
    "## 4. Encode & Decode Observations\n",
    "\n",
    "World models learn a compressed latent representation of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "encode-obs"
   },
   "outputs": [],
   "source": [
    "# Create a random observation (batch of 2 images)\n",
    "batch_size = 2\n",
    "obs = torch.randn(batch_size, 3, 64, 64, device=device)\n",
    "\n",
    "print(f\"Input observation shape: {obs.shape}\")\n",
    "\n",
    "# Encode: observation -> latent state\n",
    "with torch.no_grad():\n",
    "    state = dreamer.encode(obs)\n",
    "\n",
    "print(\"\\nLatent state components:\")\n",
    "print(f\"  Deterministic (h): {state.deterministic.shape}\")\n",
    "print(f\"  Stochastic (z): {state.stochastic.shape}\")\n",
    "print(f\"  Combined features: {state.features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "decode-state"
   },
   "outputs": [],
   "source": [
    "# Decode: latent state -> predictions\n",
    "with torch.no_grad():\n",
    "    predictions = dreamer.decode(state)\n",
    "\n",
    "print(\"Decoded predictions:\")\n",
    "for key, value in predictions.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imagination-header"
   },
   "source": [
    "## 5. Imagination Rollouts ðŸš€\n",
    "\n",
    "The core feature of world models: **predict future states without environment interaction!**\n",
    "\n",
    "This enables:\n",
    "- Planning ahead without costly environment steps\n",
    "- Learning from imagined experience\n",
    "- Model-based RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-imagination"
   },
   "outputs": [],
   "source": [
    "# Start from an initial observation\n",
    "initial_obs = torch.randn(1, 3, 64, 64, device=device)\n",
    "\n",
    "# Encode to get initial state\n",
    "with torch.no_grad():\n",
    "    initial_state = dreamer.encode(initial_obs)\n",
    "\n",
    "# Define a sequence of actions (horizon=15 steps)\n",
    "horizon = 15\n",
    "action_dim = dreamer.config.action_dim\n",
    "\n",
    "# Random action sequence: [horizon, batch_size, action_dim]\n",
    "actions = torch.randn(horizon, 1, action_dim, device=device)\n",
    "\n",
    "print(f\"Initial state features: {initial_state.features.shape}\")\n",
    "print(f\"Action sequence shape: {actions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-imagination"
   },
   "outputs": [],
   "source": [
    "# Run imagination rollout!\n",
    "with torch.no_grad():\n",
    "    trajectory = dreamer.imagine(initial_state, actions)\n",
    "\n",
    "print(\"Imagination Trajectory:\")\n",
    "print(f\"  Number of states: {len(trajectory.states)}\")\n",
    "print(f\"  Predicted rewards shape: {trajectory.rewards.shape}\")\n",
    "print(f\"  Continue probabilities shape: {trajectory.continues.shape}\")\n",
    "\n",
    "# Access individual predicted states\n",
    "print(f\"\\n  State at t=0: features {trajectory.states[0].features.shape}\")\n",
    "print(f\"  State at t=14: features {trajectory.states[-1].features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-trajectory"
   },
   "outputs": [],
   "source": [
    "# Visualize predicted rewards over the imagination horizon\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "rewards = trajectory.rewards.cpu().numpy().flatten()\n",
    "continues = trajectory.continues.cpu().numpy().flatten()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards, marker=\"o\", linewidth=2, markersize=4)\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Predicted Reward\")\n",
    "plt.title(\"Imagined Rewards\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(continues, marker=\"s\", linewidth=2, markersize=4, color=\"green\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Continue Probability\")\n",
    "plt.title(\"Episode Continuation\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-by-step-header"
   },
   "source": [
    "## 6. Step-by-Step Prediction\n",
    "\n",
    "You can also make single-step predictions with `predict()` and `observe()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "predict-step"
   },
   "outputs": [],
   "source": [
    "# predict(): Prior prediction (imagination, no observation)\n",
    "action = torch.randn(1, action_dim, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Predict next state without seeing actual observation\n",
    "    next_state_prior = dreamer.predict(initial_state, action)\n",
    "\n",
    "print(\"Prior prediction (imagination):\")\n",
    "print(f\"  Deterministic: {next_state_prior.deterministic.shape}\")\n",
    "print(f\"  Stochastic: {next_state_prior.stochastic.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "observe-step"
   },
   "outputs": [],
   "source": [
    "# observe(): Posterior update (with actual observation)\n",
    "next_obs = torch.randn(1, 3, 64, 64, device=device)  # Real observation\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Update state with actual observation (posterior)\n",
    "    next_state_posterior = dreamer.observe(initial_state, action, next_obs)\n",
    "\n",
    "print(\"Posterior update (with observation):\")\n",
    "print(f\"  Deterministic: {next_state_posterior.deterministic.shape}\")\n",
    "print(f\"  Stochastic: {next_state_posterior.stochastic.shape}\")\n",
    "\n",
    "# The posterior is more accurate because it incorporates the actual observation\n",
    "diff = (next_state_prior.stochastic - next_state_posterior.stochastic).abs().mean()\n",
    "print(f\"\\nPrior vs Posterior stochastic difference: {diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-header"
   },
   "source": [
    "## 7. Training Demo\n",
    "\n",
    "WorldLoom provides a complete training infrastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-training"
   },
   "outputs": [],
   "source": [
    "from worldloom.training import Trainer, TrainingConfig\n",
    "from worldloom.training.data import create_random_buffer\n",
    "\n",
    "# Create a fresh model for training\n",
    "model = create_world_model(\n",
    "    \"dreamerv3:size12m\",\n",
    "    obs_shape=(3, 64, 64),\n",
    "    action_dim=4,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Create a replay buffer with random data for demo\n",
    "# In practice, you'd collect real environment data\n",
    "buffer = create_random_buffer(\n",
    "    capacity=5000,\n",
    "    obs_shape=(3, 64, 64),\n",
    "    action_dim=4,\n",
    "    num_episodes=50,\n",
    "    episode_length=100,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Buffer size: {len(buffer)} transitions\")\n",
    "print(f\"Number of episodes: {buffer.num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure-training"
   },
   "outputs": [],
   "source": [
    "# Configure training (reduced for demo)\n",
    "config = TrainingConfig(\n",
    "    total_steps=500,  # Quick demo - use 50000+ for real training\n",
    "    batch_size=8,  # Small batch for Colab\n",
    "    sequence_length=16,  # Shorter sequences\n",
    "    learning_rate=3e-4,\n",
    "    log_every=100,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(model, config)\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Total steps: {config.total_steps}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Sequence length: {config.sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-training"
   },
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\\n\")\n",
    "history = trainer.train(buffer)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final loss: {history['loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-training"
   },
   "outputs": [],
   "source": [
    "# Visualize training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history[\"loss\"])\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Total Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "if \"reconstruction_loss\" in history:\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history[\"reconstruction_loss\"], label=\"Reconstruction\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Reconstruction Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "if \"kl_loss\" in history:\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history[\"kl_loss\"], color=\"orange\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"KL Divergence\")\n",
    "    plt.title(\"KL Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison-header"
   },
   "source": [
    "## 8. Model Comparison: DreamerV3 vs TD-MPC2\n",
    "\n",
    "| Feature | DreamerV3 | TD-MPC2 |\n",
    "|---------|-----------|----------|\n",
    "| **Input Type** | Images or State | State |\n",
    "| **Latent Space** | Categorical (discrete) | SimNorm (continuous) |\n",
    "| **Decoder** | Yes (reconstructs obs) | No (implicit model) |\n",
    "| **Best For** | Atari, visual tasks | MuJoCo, robotics |\n",
    "| **Planning** | Policy rollouts | MPC with Q-function |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare-models"
   },
   "outputs": [],
   "source": [
    "# Compare latent state structures\n",
    "print(\"DreamerV3 State Structure:\")\n",
    "dreamer_obs = torch.randn(1, 3, 64, 64, device=device)\n",
    "with torch.no_grad():\n",
    "    dreamer_state = dreamer.encode(dreamer_obs)\n",
    "print(f\"  Deterministic (RSSM hidden): {dreamer_state.deterministic.shape}\")\n",
    "print(f\"  Stochastic (categorical): {dreamer_state.stochastic.shape}\")\n",
    "print(\"  Has decoder: Yes\")\n",
    "\n",
    "print(\"\\nTD-MPC2 State Structure:\")\n",
    "tdmpc_obs = torch.randn(1, 39, device=device)\n",
    "with torch.no_grad():\n",
    "    tdmpc_state = tdmpc.encode(tdmpc_obs)\n",
    "print(f\"  Deterministic (SimNorm): {tdmpc_state.deterministic.shape}\")\n",
    "print(f\"  Stochastic: {tdmpc_state.stochastic}\")\n",
    "print(\"  Has decoder: No (implicit model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-load-header"
   },
   "source": [
    "## 9. Save & Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-model"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save the trained model\n",
    "save_path = \"./my_world_model\"\n",
    "model.save_pretrained(save_path)\n",
    "print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(save_path):\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = create_world_model(save_path, device=device)\n",
    "print(f\"Loaded model type: {type(loaded_model).__name__}\")\n",
    "print(f\"Loaded model config: {loaded_model.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Congratulations! You've learned the basics of WorldLoom. Here are some next steps:\n",
    "\n",
    "### Learn More\n",
    "- [Full Documentation](https://worldloom.readthedocs.io)\n",
    "- [GitHub Repository](https://github.com/yoshihyoda/worldloom)\n",
    "- [Examples Directory](https://github.com/yoshihyoda/worldloom/tree/main/examples)\n",
    "\n",
    "### Try These\n",
    "1. **Train on real data**: Collect trajectories from Atari or MuJoCo\n",
    "2. **Use larger models**: Try `dreamerv3:size50m` or `tdmpc2:48m`\n",
    "3. **Build an agent**: Use imagination for planning\n",
    "\n",
    "### Example Commands\n",
    "```bash\n",
    "# Collect Atari data\n",
    "python examples/collect_atari.py --env Breakout --episodes 100\n",
    "\n",
    "# Train DreamerV3\n",
    "python examples/train_atari_dreamer.py --data atari_data.npz --steps 100000\n",
    "\n",
    "# Visualize imagination\n",
    "python examples/visualize_imagination.py --model ./outputs/model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "**Questions or Issues?** Open an issue on [GitHub](https://github.com/yoshihyoda/worldloom/issues)\n",
    "\n",
    "**Want to contribute?** Check out our [Contributing Guide](https://github.com/yoshihyoda/worldloom/blob/main/CONTRIBUTING.md)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
